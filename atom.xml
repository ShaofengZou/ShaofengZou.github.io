<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Shaofengzou&#39;s Blog</title>
  
  <subtitle>Coding and debug</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.shaofengzou.com/"/>
  <updated>2019-12-21T04:16:31.645Z</updated>
  <id>http://www.shaofengzou.com/</id>
  
  <author>
    <name>Shaofeng Zou</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Keras Minist Recognition</title>
    <link href="http://www.shaofengzou.com/2019/12/21/Keras-Minist-Recognition/"/>
    <id>http://www.shaofengzou.com/2019/12/21/Keras-Minist-Recognition/</id>
    <published>2019-12-21T03:26:04.000Z</published>
    <updated>2019-12-21T04:16:31.645Z</updated>
    
    <content type="html"><![CDATA[<p><strong>Keras Minist Recognition 手写数字体识别</strong></p><p><strong>Author:</strong> ShaofengZou </p><p><strong>Kaggle ID:</strong> 1969608</p><p><strong>Contact:</strong> zousf19@mails.tsinghua.edu.cn</p><h3 id="I-Introduction"><a href="#I-Introduction" class="headerlink" title="I. Introduction"></a>I. Introduction</h3><img src="http://ww1.sinaimg.cn/large/007IF2qIly1ga46xlwf54j30a70a3gle.jpg" style="zoom:50%;"><p><strong>Keras Inplement Using CNN</strong></p><p><strong>Architecture:</strong> </p><p>Input -&gt; [ Conv2D -&gt; BN -&gt; Relu -&gt; Conv2D -&gt; BN -&gt; Relu -&gt; Maxpooling2D -&gt; Dropout ] * 2  -&gt; Flatten -&gt; Dense -&gt; Dropout -&gt; Output</p><p><strong>Accuracy on validation set:</strong> 99.43%</p><p><strong>Accuracy on test set:</strong> 99.614%</p><p><strong>最佳准确率(Kaggle平台给出):</strong> 99.614%</p><p><strong>提交总次数:</strong> 1</p><p><strong>Quick Start:</strong></p><p>if you work with windows or ubuntu:</p><blockquote><p>python main.py</p></blockquote><p>if you work on ubuntu with 8 GPUS, you can do multiply experiments one time:</p><blockquote><p>sh do_exp.sh</p></blockquote><p>you select the specific gpu with gpu_id</p><p><strong>If you like this project, welcome to fork and star  ^_^</strong> </p><h3 id="I-设计方案"><a href="#I-设计方案" class="headerlink" title="I. 设计方案"></a>I. 设计方案</h3><h4 id="i-数据准备"><a href="#i-数据准备" class="headerlink" title="i. 数据准备"></a>i. 数据准备</h4><ol><li>读取数据</li><li>数据归一化<ul><li>CNN对0-1的收敛更快</li></ul></li><li>数据reshape</li><li>对标签进行one-hot编码</li><li>拆分训练集和测试集</li></ol><h4 id="ii-构建CNN模型"><a href="#ii-构建CNN模型" class="headerlink" title="ii. 构建CNN模型"></a>ii. 构建CNN模型</h4><ol><li><p>设计CNN模型</p><img src="http://ww1.sinaimg.cn/large/007IF2qIly1ga46s9e3emj306w1hg76d.jpg" alt="weights" style="zoom:40%;"></li><li><p>选择优化器和设计回调函数</p><ul><li>使用RMSprop作为优化器</li><li>设计回调函数<ul><li>ReduceLROnPlateau<pre><code>* ModelCheckpoint</code></pre><ul><li>EarlyStopping</li></ul></li></ul></li></ul></li><li><p>数据集增强</p><ul><li>旋转</li><li>缩放</li><li>横向、纵向平移</li></ul></li></ol><h4 id="iii-CNN模型训练和测试"><a href="#iii-CNN模型训练和测试" class="headerlink" title="iii. CNN模型训练和测试"></a>iii. CNN模型训练和测试</h4><ol><li><p>训练时损失和准确率变化曲线</p><img src="http://ww1.sinaimg.cn/large/007IF2qIly1ga46s9bbvbj30xc0dw0tw.jpg" alt="cm-Exp_test"></li></ol><ol start="2"><li><p>在验证集上的准确率：0.9943</p><p>在验证集上的混淆矩阵：</p><img src="http://ww1.sinaimg.cn/large/007IF2qIly1ga46s9b9spj30hs0dcaan.jpg" alt="cm-Exp_test" style="zoom:67%;"></li></ol><h4 id="iv-预测和提交预测结果"><a href="#iv-预测和提交预测结果" class="headerlink" title="iv. 预测和提交预测结果"></a>iv. 预测和提交预测结果</h4><ul><li>在Kaggle上的提交结果：</li></ul><img src="http://ww1.sinaimg.cn/large/007IF2qIly1ga46s9eh0jj31h60jrk0s.jpg" alt="kaggle1" style="zoom:60%;"><ul><li>在Kaggle上的排名：</li></ul><img src="http://ww1.sinaimg.cn/large/007IF2qIly1ga46s9bg11j30x10453yi.jpg" alt="kaggle2" style="zoom:60%;"><h3 id="II-对比实验"><a href="#II-对比实验" class="headerlink" title="II. 对比实验"></a>II. 对比实验</h3><h4 id="i-参数选择"><a href="#i-参数选择" class="headerlink" title="i. 参数选择"></a>i. 参数选择</h4><p>设计对比实验，观察对比卷积核大小、数量、卷积层数、参数初始化、是否归一化等参数对模型在验证集上的准确率进行验证。</p><p>对比实验中的baseline的基本结构为：</p><p>Input -&gt; [ Conv2D -&gt; Relu -&gt; Conv2D -&gt; Relu -&gt; Maxpooling2D -&gt; Dropout ] * 2  -&gt; Flatten -&gt; Dense -&gt; Dropout -&gt; Output</p><p>这里：</p><ul><li>记block number为[Conv2D -&gt; Relu -&gt; Conv2D -&gt; Relu -&gt; Maxpooling2D -&gt; Dropout]结构的数量</li><li>记filter number为Conv2D中的卷积核数量，即输出feature map的数量</li><li>记kernel size为Conv2D中卷积核的大小</li><li>记initializers为Conv2D中权重初始化的方法<ul><li>random_normal为正态分布初始化方法，均值为0，方差为0.05</li><li>random_uniform为均匀分布初始化方法，均匀分布下边界和均匀分布上边界分别为-0.05和0.05</li><li>orthogonal为随机正交矩阵初始化方法，正交矩阵的乘性系数为1</li></ul></li></ul><table><thead><tr><th>Experiment id</th><th>block number</th><th>filter number</th><th>kernel size</th><th>initializers</th><th>batch normalization</th><th>accuracy of validation dataset</th></tr></thead><tbody><tr><td>baseline</td><td>2</td><td>32</td><td>5</td><td>random_normal</td><td>no</td><td>99.38%</td></tr><tr><td>block number - 1</td><td><strong>1</strong></td><td>32</td><td>5</td><td>random_normal</td><td>no</td><td>98.93%</td></tr><tr><td>block number - 3</td><td><strong>3</strong></td><td>32</td><td>5</td><td>random_normal</td><td>no</td><td>99.45%</td></tr><tr><td>filter number - 16</td><td>2</td><td><strong>16</strong></td><td>5</td><td>random_normal</td><td>no</td><td>99.40%</td></tr><tr><td>filter number - 64</td><td>2</td><td><strong>64</strong></td><td>5</td><td>random_normal</td><td>no</td><td>98.36%</td></tr><tr><td>kernel size - 3</td><td>2</td><td>32</td><td><strong>3</strong></td><td>random_normal</td><td>no</td><td>99.36%</td></tr><tr><td>kernel size - 7</td><td>2</td><td>32</td><td><strong>7</strong></td><td>random_normal</td><td>no</td><td>99.36%</td></tr><tr><td>initializers - random uniform</td><td>2</td><td>32</td><td>5</td><td><strong>random_uniform</strong></td><td>no</td><td>99.24%</td></tr><tr><td>initializers - orthogonal</td><td>2</td><td>32</td><td>5</td><td><strong>orthogonal</strong></td><td>no</td><td>99.31%</td></tr><tr><td>batch normalization - yes</td><td>2</td><td>32</td><td>5</td><td>random_normal</td><td><strong>yes</strong></td><td>99.45%</td></tr></tbody></table><h4 id="ii-loss变化和accuracy变化曲线"><a href="#ii-loss变化和accuracy变化曲线" class="headerlink" title="ii. loss变化和accuracy变化曲线"></a>ii. loss变化和accuracy变化曲线</h4><ul><li><p>baseline</p><img src="http://ww1.sinaimg.cn/large/007IF2qIly1ga46s9bbvbj30xc0dw0tw.jpg" alt="cm-Exp_test"></li><li><p>block number - 1</p></li></ul><img src="http://ww1.sinaimg.cn/large/007IF2qIly1ga46uttsm8j30xc0dwmyd.jpg" alt="cm-Exp_BlockNum1"><ul><li>block number - 3</li></ul><img src="http://ww1.sinaimg.cn/large/007IF2qIly1ga46uyhcduj30xc0dwdgy.jpg"><ul><li><p>filter number - 16</p><img src="http://ww1.sinaimg.cn/large/007IF2qIly1ga46vcmjl6j30xc0dwmy8.jpg"></li><li><p>filter number - 64</p><img src="http://ww1.sinaimg.cn/large/007IF2qIly1ga46vfryf8j30xc0dw0u3.jpg"></li><li><p>kernel size - 3</p><img src="http://ww1.sinaimg.cn/large/007IF2qIly1ga46vjgf3sj30xc0dwab6.jpg"></li><li><p>kernel size - 7</p><img src="http://ww1.sinaimg.cn/large/007IF2qIly1ga46vp5n7tj30xc0dwab8.jpg"></li><li><p>initializers - random uniform</p><img src="http://ww1.sinaimg.cn/large/007IF2qIly1ga46vt6s76j30xc0dwt9w.jpg"></li><li><p>initializers - orthogonal</p><img src="http://ww1.sinaimg.cn/large/007IF2qIly1ga46vx7qopj30xc0dw0tw.jpg"></li><li><p>batch normalization - yes</p><img src="http://ww1.sinaimg.cn/large/007IF2qIly1ga46w1dl4yj30xc0dwt9w.jpg"></li></ul><h4 id="iii-对比实验总结"><a href="#iii-对比实验总结" class="headerlink" title="iii. 对比实验总结"></a>iii. 对比实验总结</h4><ol><li><p>卷积层数太少会降低模型精度，层数太多的话模型复杂度会增加</p></li><li><p>卷积核数量过大会难以收敛，训练中波动会较大</p></li><li><p>添加批量归一化可以使模型收敛更快，因为批量归一化可以解决梯度弥散的问题</p></li></ol><h3 id="III-问题思考"><a href="#III-问题思考" class="headerlink" title="III. 问题思考"></a>III. 问题思考</h3><ol><li><p>实验训练什么时候停止是最合适的？简要陈述你的实现方式，并试分析固定迭代次数与通过验证集调整等方法的优缺点。</p><p>Answer:</p><ul><li>在验证集的损失不再上升时，可以停止训练。通过训练过程中设置EarlyStopping的回调函数，当多次训练后的验证集损失不再降低时，就停止训练。同时，设置ReduceLROnPlateau的回调函数，当多次训练后的验证集准确度不再上升时，降低学习率。</li><li>固定迭代次数的缺点就是无法确定一个合适的迭代次数，迭代次数设置的过大则会浪费训练时间，过小则无法获得最优模型。</li></ul></li><li><p>实验参数的初始化是怎么做的？不同的方法适合哪些地方？ </p><p>Answer: </p><ul><li>使用零均值初始化，高斯分布初始化，正交初始化等方法初始化等方法。</li><li>不可以采用零初始化，因为这样在每次迭代过程中所有权重的变化都会是一样的。</li><li>不可以采用过大或过小的初值，因为卷积之后的值会过大过小，在激活函数附近的梯度就会几乎为0，产生梯度消失的问题。</li></ul></li><li><p>过拟合是深度学习常见的问题，有什么方法可以方式训练过程陷入过拟合？</p><p>Answer:</p><p>可以尝试使用：</p><ul><li>Batch-Normalization</li><li>Dropout</li><li>Early-Stopping</li></ul></li><li><p>试分析CNN（卷积神经网络）相对于全连接神经网络的优点</p><ul><li>全连接神经网络的参数过多，所以计算速度慢，且容易引起过拟合。</li><li>CNN相对于全连接神经网络的特点是局部链接、权值共享、引入池化可降低特征图大小，所以具有的优点为：<ul><li>参数少，只与卷积核的大小和数量有关</li><li>具有特征抽取能力</li><li>特征的平移不变性</li></ul></li></ul></li></ol><h3 id="IV-心得体会"><a href="#IV-心得体会" class="headerlink" title="IV. 心得体会"></a>IV. 心得体会</h3><ol><li>Batch-Normalization可以提升训练时的稳定性，解决梯度弥散问题</li><li>为了防止过拟合，可以进行适当的数据增强</li><li>防止过拟合，还可以设计回调函数，根据验证集的测试精度来降低学习率或者停止迭代，并根据验证集的准确率来保存最佳的模型</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;Keras Minist Recognition 手写数字体识别&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Author:&lt;/strong&gt; ShaofengZou &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kaggle ID:&lt;/strong&gt; 1969608
      
    
    </summary>
    
    
    
      <category term="Keras Minist Recognition 手写数字体识别" scheme="http://www.shaofengzou.com/tags/Keras-Minist-Recognition-%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E4%BD%93%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>刘亦菲</title>
    <link href="http://www.shaofengzou.com/2019/09/26/%E5%88%98%E4%BA%A6%E8%8F%B2/"/>
    <id>http://www.shaofengzou.com/2019/09/26/刘亦菲/</id>
    <published>2019-09-26T11:48:34.000Z</published>
    <updated>2019-12-21T03:39:12.967Z</updated>
    
    <content type="html"><![CDATA[<h3 id="oh-my-God"><a href="#oh-my-God" class="headerlink" title="oh my God"></a>oh my God</h3><a id="more"></a><p align="center"> <img src="http://ww1.sinaimg.cn/large/007IF2qIly1g9wbe4fmgbj31hc0xctoz.jpg"></p><p align="center"> <img src="http://ww1.sinaimg.cn/large/007IF2qIly1g9wbh2fqrmj31hc0xch26.jpg"></p><p align="center"> <img src="http://ww1.sinaimg.cn/large/007IF2qIly1g9wbh2q8tgj31hc0xcqlc.jpg"></p><p align="center"> <img src="http://ww1.sinaimg.cn/large/007IF2qIly1g9wbh2g2woj31hc0xcnaf.jpg"></p><p align="center"> <img src="http://ww1.sinaimg.cn/large/007IF2qIly1g9wbh2i9c5j31hc0xc17y.jpg"></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;oh-my-God&quot;&gt;&lt;a href=&quot;#oh-my-God&quot; class=&quot;headerlink&quot; title=&quot;oh my God&quot;&gt;&lt;/a&gt;oh my God&lt;/h3&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Blind Denoising Method</title>
    <link href="http://www.shaofengzou.com/2019/09/26/Blind-Denoising-Method/"/>
    <id>http://www.shaofengzou.com/2019/09/26/Blind-Denoising-Method/</id>
    <published>2019-09-26T11:23:08.000Z</published>
    <updated>2019-09-26T12:10:54.735Z</updated>
    
    <content type="html"><![CDATA[<h2 id="A-CNN-Based-Blind-Denoising-Method-for-Endoscopic-Images"><a href="#A-CNN-Based-Blind-Denoising-Method-for-Endoscopic-Images" class="headerlink" title="A CNN-Based Blind Denoising Method for Endoscopic Images"></a>A CNN-Based Blind Denoising Method for Endoscopic Images</h2><a id="more"></a><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p align="center"> <img src="http://i1.fuimg.com/700288/6b77caa83f0d8804.png" width="50%"></p><h2 id="Blind-Denosing-Network"><a href="#Blind-Denosing-Network" class="headerlink" title="Blind Denosing Network"></a>Blind Denosing Network</h2><p align="center"> <img src="http://i1.fuimg.com/700288/312d0425c58480fe.png" width="50%"></p><h2 id="Blind-Image-Quality-Assessment-Network"><a href="#Blind-Image-Quality-Assessment-Network" class="headerlink" title="Blind Image Quality Assessment Network"></a>Blind Image Quality Assessment Network</h2><p align="center"> <img src="http://i2.tiimg.com/700288/e46fc902144e04b5.jpg" width="50%"></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote><p>The quality of images captured by wireless capsule endoscopy (WCE) is key for doctors to diagnose diseases of gastrointestinal (GI) tract. However, there exist many low-quality endoscopic images due to the limited illumination and complex environment in GI tract. After an enhancement process, the severe noise become an unacceptable problem. The noise varies with different cameras, GI tract environments and image enhancement. And the noise model is hard to be obtained. This paper proposes a convolutional blind denoising network for endoscopic images. We apply Deep Image Prior (DIP) method to reconstruct a clean image iteratively using a noisy image without a specific noise model and ground truth. Then we design a blind image quality assessment network based on MobileNet to estimate the quality of the reconstructed images. The estimated quality is used to stop the iterative operation in DIP method. The number of iterations is reduced about 36% by using transfer learning in our DIP process. Experimental results on endoscopic images and real-world noisy images demonstrate the superiority of our proposed method over the state-of-the-art methods in terms of visual quality and quantitative metrics.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;A-CNN-Based-Blind-Denoising-Method-for-Endoscopic-Images&quot;&gt;&lt;a href=&quot;#A-CNN-Based-Blind-Denoising-Method-for-Endoscopic-Images&quot; class=&quot;headerlink&quot; title=&quot;A CNN-Based Blind Denoising Method for Endoscopic Images&quot;&gt;&lt;/a&gt;A CNN-Based Blind Denoising Method for Endoscopic Images&lt;/h2&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Android Style Transfer</title>
    <link href="http://www.shaofengzou.com/2019/09/26/Android-Style-Transfer/"/>
    <id>http://www.shaofengzou.com/2019/09/26/Android-Style-Transfer/</id>
    <published>2019-09-26T09:12:47.000Z</published>
    <updated>2019-12-21T03:48:04.808Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Android-Style-Transfer"><a href="#Android-Style-Transfer" class="headerlink" title="Android-Style-Transfer"></a>Android-Style-Transfer</h2><p>An Android app built with an artistic style transfer neural network</p><a id="more"></a><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Artistic style transfer help you creat exciting image with sytle you like. It need two input images: one representing the artistic style and one representing the content.</p><p align="center"> <img src="http://ww1.sinaimg.cn/large/007IF2qIly1ga478npx6xj319z0ftx5m.jpg" width="50%"></p><p>This project implement style transfer on Android platfrom. It contains two parts including training style model on computer using python and deploy the model to the moblie phone with Android Studio.</p><p>The app can be download via <a href="https://pan.baidu.com/s/10pMQnKWoPmTdra1GovThqg" target="_blank" rel="noopener">BaiduCloud</a></p><p align="center"> <img src="http://ww1.sinaimg.cn/large/007IF2qIly1ga477txrnnj30tr1lzkjl.jpg" width="30%"></p><h2 id="Part-I-Style-Model-Training"><a href="#Part-I-Style-Model-Training" class="headerlink" title="Part I. Style Model Training"></a>Part I. Style Model Training</h2><h3 id="Step1-Download-dataset-and-pretrained-VGG-model"><a href="#Step1-Download-dataset-and-pretrained-VGG-model" class="headerlink" title="Step1. Download dataset and pretrained VGG model"></a>Step1. Download dataset and pretrained VGG model</h3><ul><li><a href="http://msvocds.blob.core.windows.net/coco2014/train2014.zips" target="_blank" rel="noopener">COCO dataset (about 12GB)</a>    </li><li><a href="http://www.vlfeat.org/matconvnet/pretrained/" target="_blank" rel="noopener">VGG pre-trained model</a>    </li></ul><p>Unzip the train2014.zip and put the folder <code>train2014</code> and <code>imagenet-vgg-verydeep-19.mat</code> to the <code>data</code> folder</p><h3 id="Step2-Train-model"><a href="#Step2-Train-model" class="headerlink" title="Step2. Train model"></a>Step2. Train model</h3><p>The training code refers to <a href="https://github.com/GuidoPaul/Android-Tensorflow-Style-Transfer" target="_blank" rel="noopener">@GuidoPaul</a></p><p><code>python style-transfer.py --style images/styles/wave.jpg --test images/test/test.jpg</code> <br></p><p>It takes about 3 to 4 hours to train on TITAN XP.</p><p>The trained model is saved in <strong>checkpoint</strong> folder and the converted pb model file is saved in <strong>model</strong> file.</p><p>You can download the pretrained model trained by images/styled/wave.jpg via <a href="https://pan.baidu.com/s/10pMQnKWoPmTdra1GovThqg" target="_blank" rel="noopener">BaiduCloud</a></p><h3 id="Step3-Evaluate"><a href="#Step3-Evaluate" class="headerlink" title="Step3. Evaluate"></a>Step3. Evaluate</h3><p><code>python test_pb_file.py --model_name model/style_graph_frozen.pb --test_image images/test/test2.jpg</code> <br></p><p>Run to see the input and output node and tensor size of trained model which are important for the setting on Andoird.</p><h2 id="Part-II-Android-Setup"><a href="#Part-II-Android-Setup" class="headerlink" title="Part II. Android Setup"></a>Part II. Android Setup</h2><p>After traning style model using python, we need deploy the trained model on android.</p><p>To run this project, you need put the pre-trained pb modol file to <code>Part2_Android_Project\Style_Transfer\app\src\main\assets</code> download via <a href="https://pan.baidu.com/s/10pMQnKWoPmTdra1GovThqg" target="_blank" rel="noopener">BaiduCloud</a></p><p>To add your trained model, you need to follow these steps:</p><h3 id="Step1-Add-style-image"><a href="#Step1-Add-style-image" class="headerlink" title="Step1. Add style image"></a>Step1. Add style image</h3><ol><li>Add the style image to the folder <code>Part2_Android_Project\Style_Transfer\app\src\main\res\drawable</code></li><li>Create style object to the function <code>initStyles()</code> at line <strong>653</strong> in <strong>MainActivity.java</strong></li></ol><h3 id="Step2-Add-style-model"><a href="#Step2-Add-style-model" class="headerlink" title="Step2. Add style model"></a>Step2. Add style model</h3><ol><li>Add the style model to the folder <code>Part2_Android_Project\Style_Transfer\app\src\main\assets</code></li><li>Add this model to <code>pu_list</code> at Line <strong>73</strong> in <strong>MainActivity.java</strong></li></ol><p>If you like it, welcome to <strong>fork</strong> and <strong>star</strong> ^_^</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Android-Style-Transfer&quot;&gt;&lt;a href=&quot;#Android-Style-Transfer&quot; class=&quot;headerlink&quot; title=&quot;Android-Style-Transfer&quot;&gt;&lt;/a&gt;Android-Style-Transfer&lt;/h2&gt;&lt;p&gt;An Android app built with an artistic style transfer neural network&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
</feed>
